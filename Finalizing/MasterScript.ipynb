{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d2c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared content of 't5_triplets.csv'.\n",
      "Cleared content of 'wiki_output.csv'.\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m     process_unique_topics(grouped_csv, unique_topics_file)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 135\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m clear_output_files([t5_triplets_output, unique_topics_file, wiki_csv])\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Update wiki_output.csv with missing articles\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m \u001b[43mupdate_wiki_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouped_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwiki_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Generate T5 training triplets\u001b[39;00m\n\u001b[0;32m    138\u001b[0m generate_t5_triplets(wiki_csv, grouped_csv, t5_triplets_output)\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mupdate_wiki_articles\u001b[1;34m(grouped_csv, wiki_csv)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_wiki_articles\u001b[39m(grouped_csv, wiki_csv):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetch missing Wikipedia articles from grouped data and update wiki_output.csv\"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     existing_articles \u001b[38;5;241m=\u001b[39m \u001b[43mload_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwiki_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     updated_articles \u001b[38;5;241m=\u001b[39m existing_articles\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(grouped_csv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m, in \u001b[0;36mload_articles\u001b[1;34m(article_csv)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(article_csv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# skip header\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(row) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def load_articles(article_csv):\n",
    "    \"\"\"Load wikiArticles.csv into a dictionary: {topic: article}\"\"\"\n",
    "    articles = {}\n",
    "    with open(article_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:\n",
    "                topic, article = row[0], row[1]\n",
    "                articles[topic.strip()] = article.strip()\n",
    "    return articles\n",
    "\n",
    "def update_wiki_articles(grouped_csv, wiki_csv):\n",
    "    \"\"\"Fetch missing Wikipedia articles from grouped data and update wiki_output.csv\"\"\"\n",
    "    existing_articles = load_articles(wiki_csv)\n",
    "    updated_articles = existing_articles.copy()\n",
    "    \n",
    "    with open(grouped_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        \n",
    "        for row in reader:\n",
    "            if len(row) >= 1:\n",
    "                topic = row[0].strip()\n",
    "                \n",
    "                # If the article for this topic doesn't exist, fetch it\n",
    "                if topic not in existing_articles:\n",
    "                    print(f\"Fetching article for topic '{topic}'...\")\n",
    "                    article = fetch_wikipedia_summary(topic)\n",
    "                    if article:\n",
    "                        updated_articles[topic] = article\n",
    "                    else:\n",
    "                        print(f\"Warning: No article found for topic '{topic}'\")\n",
    "    \n",
    "    # Update the wiki_output.csv with all the articles\n",
    "    with open(wiki_csv, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"topic\", \"article\"])  # Write the header\n",
    "        for topic, article in updated_articles.items():\n",
    "            writer.writerow([topic, article])\n",
    "    \n",
    "    print(f\"Updated {len(updated_articles)} articles in '{wiki_csv}'.\")\n",
    "\n",
    "def generate_t5_triplets(wiki_csv, grouped_csv, output_file):\n",
    "    \"\"\"Generate T5 training triplets and write to a new CSV.\"\"\"\n",
    "    articles = load_articles(wiki_csv)\n",
    "    triplets = []\n",
    "    \n",
    "    # Group the items in the grouped CSV by topic\n",
    "    grouped_data = {}\n",
    "    with open(grouped_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:\n",
    "                topic = row[0].strip()\n",
    "                summary = row[1].strip()\n",
    "\n",
    "                # Group the summaries by topic\n",
    "                if topic not in grouped_data:\n",
    "                    grouped_data[topic] = []\n",
    "                grouped_data[topic].append(summary)\n",
    "\n",
    "    # Now generate the triplets using the grouped summaries\n",
    "    for topic, summaries in grouped_data.items():\n",
    "        article = articles.get(topic)\n",
    "        if article:\n",
    "            for summary in summaries:\n",
    "                triplets.append([\"summarize\", article, summary])\n",
    "        else:\n",
    "            print(f\"Warning: No article found for topic '{topic}'\")\n",
    "\n",
    "    # Write the triplets to the output CSV\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"task\", \"input\", \"target\"])\n",
    "        writer.writerows(triplets)\n",
    "\n",
    "    print(f\"Generated {len(triplets)} T5 training triplets in '{output_file}'.\")\n",
    "\n",
    "def fetch_wikipedia_summary(topic):\n",
    "    \"\"\"Fetch the summary of a Wikipedia article for a given topic.\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&exintro&explaintext&format=json&titles={topic}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    for page_id, page in pages.items():\n",
    "        return page.get(\"extract\", \"\")\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def process_unique_topics(input_csv, unique_topics_file):\n",
    "    \"\"\"Process unique topics from input CSV and store in a file.\"\"\"\n",
    "    unique_topics = set()\n",
    "    \n",
    "    with open(input_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            if len(row) >= 1:\n",
    "                unique_topics.add(row[0].strip())\n",
    "    \n",
    "    # Clear and write the unique topics to the file\n",
    "    with open(unique_topics_file, 'w', encoding='utf-8') as f:\n",
    "        for topic in unique_topics:\n",
    "            f.write(f\"{topic}\\n\")\n",
    "    \n",
    "    print(f\"Processed {len(unique_topics)} unique topics and saved to '{unique_topics_file}'.\")\n",
    "\n",
    "def clear_output_files(files):\n",
    "    \"\"\"Clear content of output files before writing new data.\"\"\"\n",
    "    for file in files:\n",
    "        if os.path.exists(file):\n",
    "            with open(file, 'w', encoding='utf-8') as f:\n",
    "                pass  # Just open and close to clear content\n",
    "            print(f\"Cleared content of '{file}'.\")\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    wiki_csv = \"wiki_output.csv\"  # Store fetched articles here\n",
    "    grouped_csv = \"output_grouped.csv\"  # Do not modify this file\n",
    "    t5_triplets_output = \"t5_triplets.csv\"\n",
    "    unique_topics_file = \"unique_topics.txt\"\n",
    "    \n",
    "    # Clear previous output files (except for output_grouped.csv)\n",
    "    clear_output_files([t5_triplets_output, unique_topics_file, wiki_csv])\n",
    "    \n",
    "    # Update wiki_output.csv with missing articles\n",
    "    update_wiki_articles(grouped_csv, wiki_csv)\n",
    "    \n",
    "    # Generate T5 training triplets\n",
    "    generate_t5_triplets(wiki_csv, grouped_csv, t5_triplets_output)\n",
    "    \n",
    "    # Process unique topics\n",
    "    process_unique_topics(grouped_csv, unique_topics_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
