{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install t5\n",
    "#import requests\n",
    "#\n",
    "#subject = input(\"What is your topic?\")\n",
    "\n",
    "#url = 'https://en.wikipedia.org/w/api.php'\n",
    "#params={\n",
    "#    'action':'query',\n",
    "#    'format':'json',\n",
    "#    'titles': subject,\n",
    "#    'prop':'extracts',\n",
    "#    'exintro':True,\n",
    "#    'explaintext': True,\n",
    "#}\n",
    "#response = requests.get(url, params=params)\n",
    "#data = response.json()\n",
    "#\n",
    "#page = next(iter(data['query']['pages'].values()))\n",
    "#\n",
    "#print(page['extract'][:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 9 new articles in wikiArticles.sasv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_first_column_set_to_txt(csv_file_path, output_txt_path):\n",
    "    try:\n",
    "        # Create an empty set to store unique values\n",
    "        first_column_set = set()\n",
    "        \n",
    "        # Open and read the CSV file\n",
    "        with open(csv_file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            \n",
    "            # Iterate through each row and add first column value to set\n",
    "            for row in csv_reader:\n",
    "                if row:  # Check if row is not empty\n",
    "                    first_column_set.add(row[0])\n",
    "        \n",
    "        # Write the set to a text file\n",
    "        with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "            for item in first_column_set:\n",
    "                txt_file.write(f\"{item}\\n\")  # Write each item on a new line\n",
    "                \n",
    "        print(f\"Set successfully written to '{output_txt_path}'\")\n",
    "        return first_column_set\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_file_path}' not found.\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return set()\n",
    "\n",
    "def search_and_fetch_summary(topic):\n",
    "    \"\"\"Search Wikipedia for a topic and return parsed HTML summary of the best match (index 0).\"\"\"\n",
    "    search_url = 'https://en.wikipedia.org/w/api.php'\n",
    "\n",
    "    search_params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'search',\n",
    "        'utf8': 1,\n",
    "        'srsearch': topic\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        search_data = requests.get(search_url, params=search_params, timeout=5).json()\n",
    "        search_results = search_data.get('query', {}).get('search', [])\n",
    "        if not search_results:\n",
    "            return f\"No Wikipedia article found for topic: {topic}\"\n",
    "\n",
    "        best_match_title = search_results[0]['title']\n",
    "\n",
    "        parse_params = {\n",
    "            'action': 'parse',\n",
    "            'format': 'json',\n",
    "            'page': best_match_title,\n",
    "            'prop': 'text',\n",
    "            'redirects': ''\n",
    "        }\n",
    "\n",
    "        response = requests.get(search_url, params=parse_params, timeout=5).json()\n",
    "        raw_html = response['parse']['text']['*']\n",
    "        document = html.document_fromstring(raw_html)\n",
    "\n",
    "        text = ''\n",
    "        for p in document.xpath('//p'):\n",
    "            text += p.text_content() + '\\n'\n",
    "\n",
    "        return text.strip() if text.strip() else f\"No extractable summary found for {best_match_title}.\"\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Request error while fetching summary for {topic}: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error for {topic}: {e}\"\n",
    "\n",
    "def load_processed_topics(filename):\n",
    "    \"\"\"Loads previously processed topics from a file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "                return set(line.strip() for line in file if line.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading processed topics from '{filename}': {e}\")\n",
    "    return set()\n",
    "\n",
    "def save_processed_topic(filename, topic):\n",
    "    \"\"\"Saves a topic to the processed topics file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"a\", encoding=\"utf-8\") as file:\n",
    "            file.write(topic + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving topic '{topic}': {e}\")\n",
    "\n",
    "def write_to_sasv(filename, data_list):\n",
    "    \"\"\"Writes a list of unique Wikipedia summaries to a .sasv file, separating entries with ' /* '.\"\"\"\n",
    "    full_filename = f\"{filename}.sasv\"\n",
    "    try:\n",
    "        with open(full_filename, \"a\", encoding=\"utf-8\") as file:\n",
    "            file.write(\" /* \".join(data_list) + \" /* \")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to '{full_filename}': {e}\")\n",
    "\n",
    "def process_csv(csv_filename, sasv_filename, processed_filename, txt_filename):\n",
    "    \"\"\"Processes the first column of a CSV file, fetches Wikipedia summaries, and stores them uniquely.\"\"\"\n",
    "    # Get the set of unique topics from the CSV file\n",
    "    unique_topics = get_first_column_set_to_txt(csv_filename, txt_filename)\n",
    "    if not unique_topics:\n",
    "        print(\"No topics found in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    processed_topics = load_processed_topics(processed_filename)\n",
    "    new_summaries = []\n",
    "\n",
    "    for topic in unique_topics:\n",
    "        if topic and topic not in processed_topics:\n",
    "            summary = search_and_fetch_summary(topic)\n",
    "            if summary:\n",
    "                new_summaries.append(summary)\n",
    "                save_processed_topic(processed_filename, topic)\n",
    "\n",
    "    if new_summaries:\n",
    "        write_to_sasv(sasv_filename, new_summaries)\n",
    "        print(f\"Stored {len(new_summaries)} new articles in {sasv_filename}.sasv\")\n",
    "    else:\n",
    "        print(\"No new unique topics found.\")\n",
    "\n",
    "# Call the main function\n",
    "process_csv(\"csv/output.csv\", \"wikiArticles\", \"unique_topics.txt\", \"unique_topics.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#untested\n",
    "def create_t5_triplets(csv_filename):\n",
    "    \"\"\"\n",
    "    Creates T5-style triplets for summarization based on the input CSV.\n",
    "    Each triplet consists of (Input, Target), where:\n",
    "    - Input is the task with the article content.\n",
    "    - Target is the corresponding summary.\n",
    "\n",
    "    Args:\n",
    "    - csv_filename (str): The path to the CSV file that contains the article and its summary.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of triplets in the form of (Input, Target).\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    \n",
    "    try:\n",
    "        with open(csv_filename, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)  # Skip the header row\n",
    "            \n",
    "            for row in reader:\n",
    "                if row and len(row) >= 2:\n",
    "                    article_content = row[0]  # Wikipedia article content (Column 1)\n",
    "                    summary = row[1]  # Summary (Column 2)\n",
    "                    \n",
    "                    # Creating the triplet\n",
    "                    input_triplet = f\"Summarize: {article_content}\"\n",
    "                    target_triplet = summary\n",
    "                    \n",
    "                    # Append the triplet to the list\n",
    "                    triplets.append((input_triplet, target_triplet))\n",
    "        \n",
    "        print(f\"Created {len(triplets)} triplets successfully.\")\n",
    "        return triplets\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_filename}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "csv_file = \"csv/output.csv\"  \n",
    "triplets = create_t5_triplets(csv_file)\n",
    "\n",
    "# Print the first few triplets for confirmation\n",
    "for i, (input_t, target_t) in enumerate(triplets[:5]):\n",
    "    print(f\"Triplet {i+1} - Input: {input_t}\\nTarget: {target_t}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#untested\n",
    "def sort_csv_by_category(csv_filename, sorted_csv_filename):\n",
    "    \"\"\"\n",
    "    Sorts the CSV file by the category in the first column and saves the result to a new file.\n",
    "    \n",
    "    Args:\n",
    "    - csv_filename (str): The path to the original CSV file.\n",
    "    - sorted_csv_filename (str): The path where the sorted CSV file will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if sorting and saving was successful, False if there was an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        with open(csv_filename, newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        # Skip the header\n",
    "        header = rows[0]\n",
    "        data_rows = rows[1:]\n",
    "        \n",
    "        # Sort the rows based on the first column (category)\n",
    "        data_rows.sort(key=lambda x: x[0])  # Sorting by the first column (category)\n",
    "        \n",
    "        # Write the sorted rows into a new CSV file\n",
    "        with open(sorted_csv_filename, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow(header)  # Write the header\n",
    "            writer.writerows(data_rows)  # Write the sorted rows\n",
    "            \n",
    "        print(f\"CSV file sorted and saved to '{sorted_csv_filename}' successfully.\")\n",
    "        return True\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_filename}' not found.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "csv_file = \"csv/output.csv\"  \n",
    "sorted_csv_file = \"csv/sorted_output.csv\"  \n",
    "sort_csv_by_category(csv_file, sorted_csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
