{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install t5\n",
    "#import requests\n",
    "#\n",
    "#subject = input(\"What is your topic?\")\n",
    "\n",
    "#url = 'https://en.wikipedia.org/w/api.php'\n",
    "#params={\n",
    "#    'action':'query',\n",
    "#    'format':'json',\n",
    "#    'titles': subject,\n",
    "#    'prop':'extracts',\n",
    "#    'exintro':True,\n",
    "#    'explaintext': True,\n",
    "#}\n",
    "#response = requests.get(url, params=params)\n",
    "#data = response.json()\n",
    "#\n",
    "#page = next(iter(data['query']['pages'].values()))\n",
    "#\n",
    "#print(page['extract'][:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_wikipedia_summary(topic):\n",
    "    \"\"\"Fetches the summary of a Wikipedia article for a given topic.\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{topic}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"extract\", \"No summary available.\")\n",
    "    return None\n",
    "\n",
    "def read_first_column(csv_filename):\n",
    "    \"\"\"Reads the first column from a CSV file and returns a list of topics.\"\"\"\n",
    "    topics = []\n",
    "    with open(csv_filename, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                topics.append(row[0])\n",
    "    return topics\n",
    "\n",
    "def load_processed_topics(filename):\n",
    "    \"\"\"Loads previously processed topics from a file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            return set(file.read().splitlines())\n",
    "    return set()\n",
    "\n",
    "def save_processed_topic(filename, topic):\n",
    "    \"\"\"Saves a topic to the processed topics file.\"\"\"\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(topic + \"\\n\")\n",
    "\n",
    "def write_to_sasv(filename, data_list):\n",
    "    \"\"\"Writes a list of unique Wikipedia summaries to a .sasv file, separating entries with '/*'.\"\"\"\n",
    "    full_filename = f\"{filename}.sasv\"\n",
    "    with open(full_filename, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(\" /* \".join(data_list) + \" /* \")\n",
    "\n",
    "def process_csv(csv_filename, sasv_filename, processed_filename):\n",
    "    \"\"\"Processes the first column of a CSV file, fetches Wikipedia summaries, and stores them uniquely.\"\"\"\n",
    "    topics = read_first_column(csv_filename)\n",
    "    processed_topics = load_processed_topics(processed_filename)\n",
    "    new_summaries = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        if topic not in processed_topics:\n",
    "            summary = get_wikipedia_summary(topic)\n",
    "            if summary:\n",
    "                new_summaries.append(summary)\n",
    "                save_processed_topic(processed_filename, topic)\n",
    "    \n",
    "    if new_summaries:\n",
    "        write_to_sasv(sasv_filename, new_summaries)\n",
    "        print(f\"Stored {len(new_summaries)} new articles in {sasv_filename}.sasv\")\n",
    "    else:\n",
    "        print(\"No new unique topics found.\")\n",
    "\n",
    "# Example usage\n",
    "#process_csv(\"topics.csv\", \"articles\", \"processed_topics.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
