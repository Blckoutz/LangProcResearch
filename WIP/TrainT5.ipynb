{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1365e54a-24f0-42f7-8cc3-2fbb5ab4ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = [\n",
    "    [\"binary classification\", \"Anakin was Luke's father\" , 1],\n",
    "    [\"binary classification\", \"Luke was a Sith Lord\" , 0],\n",
    "    [\"generate question\", \"Star Wars is an American epic space-opera media franchise created by George Lucas, which began with the eponymous 1977 film and quickly became a worldwide pop-culture phenomenon\", \"Who created the Star Wars franchise?\"],\n",
    "    [\"generate question\", \"Anakin was Luke's father\" , \"Who was Luke's father?\"],\n",
    "]\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"prefix\", \"input_text\", \"target_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cead1f6c-dd82-4eca-805a-58f220bae58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    [\"binary classification\", \"Leia was Luke's sister\" , 1],\n",
    "    [\"binary classification\", \"Han was a Sith Lord\" , 0],\n",
    "    [\"generate question\", \"In 2020, the Star Wars franchise's total value was estimated at US$70 billion, and it is currently the fifth-highest-grossing media franchise of all time.\", \"What is the total value of the Star Wars franchise?\"],\n",
    "    [\"generate question\", \"Leia was Luke's sister\" , \"Who was Luke's sister?\"],\n",
    "]\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"prefix\", \"input_text\", \"target_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ecc311-e4a0-419d-a19e-ddd70ac20d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = [\n",
    "    \"binary classification: Luke blew up the first Death Star\",\n",
    "    \"generate question: In 1971, George Lucas wanted to film an adaptation of the Flash Gordon serial, but could not obtain the rights, so he began developing his own space opera.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fbf9a1-86a3-46cf-975d-6b1d4bbc95b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4, 3)\n",
      "                  prefix                                         input_text  \\\n",
      "0  binary classification                           Anakin was Luke's father   \n",
      "1  binary classification                               Luke was a Sith Lord   \n",
      "2      generate question  Star Wars is an American epic space-opera medi...   \n",
      "3      generate question                           Anakin was Luke's father   \n",
      "\n",
      "                            target_text  \n",
      "0                                     1  \n",
      "1                                     0  \n",
      "2  Who created the Star Wars franchise?  \n",
      "3                Who was Luke's father?  \n",
      "                  prefix                                         input_text  \\\n",
      "0  binary classification                             Leia was Luke's sister   \n",
      "1  binary classification                                Han was a Sith Lord   \n",
      "2      generate question  In 2020, the Star Wars franchise's total value...   \n",
      "3      generate question                             Leia was Luke's sister   \n",
      "\n",
      "                                         target_text  \n",
      "0                                                  1  \n",
      "1                                                  0  \n",
      "2  What is the total value of the Star Wars franc...  \n",
      "3                             Who was Luke's sister?  \n",
      "   prefix  input_text  target_text\n",
      "0      21          24            1\n",
      "1      21          20            1\n",
      "2      17         177           36\n",
      "3      17          24           22\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   prefix       4 non-null      object\n",
      " 1   input_text   4 non-null      object\n",
      " 2   target_text  4 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 224.0+ bytes\n",
      "None\n",
      "                  prefix                                         input_text  \\\n",
      "0  binary classification                           Anakin was Luke's father   \n",
      "1  binary classification                               Luke was a Sith Lord   \n",
      "2      generate question  Star Wars is an American epic space-opera medi...   \n",
      "3      generate question                           Anakin was Luke's father   \n",
      "\n",
      "                            target_text  \n",
      "0                                     1  \n",
      "1                                     0  \n",
      "2  Who created the Star Wars franchise?  \n",
      "3                Who was Luke's father?  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18458\\AppData\\Local\\Temp\\ipykernel_26232\\659197699.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(train_df.applymap(lambda x: len(x) if isinstance(x, (list, str)) else 1))\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf725851c4b4714b81c41f87766fabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1284\n",
      "INFO:simpletransformers.t5.t5_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d401914321e24513b462518721d66061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b83d2150474a379a66a92409699c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 200:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bef13b7020042a1b78117f7d5208ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1284\n",
      "INFO:simpletransformers.t5.t5_model:{'eval_loss': 3.138669729232788}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(preds)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m pred \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels, preds)])\n\u001b[1;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount_matches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39meval_model(eval_df, matches\u001b[38;5;241m=\u001b[39mcount_matches))\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m##Try pip install --upgrade transformers simpletransformers\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:242\u001b[0m, in \u001b[0;36mT5Model.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, qrels, run_dict, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(train_data, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    240\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:803\u001b[0m, in \u001b[0;36mT5Model.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_data, qrels, run_dict, eval_name, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    802\u001b[0m     training_progress_scores[key]\u001b[38;5;241m.\u001b[39mappend(results[key])\n\u001b[1;32m--> 803\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_progress_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m report\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m    805\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_progress_scores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    806\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    807\u001b[0m )\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mwandb_project \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sweeping:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# pip install simpletransformers\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", legacy=False)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "train_data = [\n",
    "    [\"binary classification\", \"Anakin was Luke's father\" , \"1\"],\n",
    "    [\"binary classification\", \"Luke was a Sith Lord\" , \"0\"],\n",
    "    [\"generate question\", \"Star Wars is an American epic space-opera media franchise created by George Lucas, which began with the eponymous 1977 film and quickly became a worldwide pop-culture phenomenon\", \"Who created the Star Wars franchise?\"],\n",
    "    [\"generate question\", \"Anakin was Luke's father\" , \"Who was Luke's father?\"],\n",
    "]\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"prefix\", \"input_text\", \"target_text\"]\n",
    "\n",
    "\n",
    "eval_data = [\n",
    "    [\"binary classification\", \"Leia was Luke's sister\" , \"1\"],\n",
    "    [\"binary classification\", \"Han was a Sith Lord\" , \"0\"],\n",
    "    [\"generate question\", \"In 2020, the Star Wars franchise's total value was estimated at US$70 billion, and it is currently the fifth-highest-grossing media franchise of all time.\", \"What is the total value of the Star Wars franchise?\"],\n",
    "    [\"generate question\", \"Leia was Luke's sister\" , \"Who was Luke's sister?\"],\n",
    "]\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df.columns = [\"prefix\", \"input_text\", \"target_text\"]\n",
    "\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].astype(str)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].astype(str)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(eval_df.shape)\n",
    "print(train_df.head())\n",
    "print(eval_df.head())\n",
    "\n",
    "print(train_df.applymap(lambda x: len(x) if isinstance(x, (list, str)) else 1))\n",
    "\n",
    "print(train_df.info())\n",
    "print(train_df.head())\n",
    "\n",
    "#model_args = T5Args()\n",
    "#model_args.num_train_epochs = 200\n",
    "#model_args.no_save = True\n",
    "#model_args.evaluate_generated_text = True\n",
    "#model_args.evaluate_during_training = True\n",
    "#model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "#model_args.use_cuda = False #modify for running in 260\n",
    "#model = T5Model(\"t5\", \"t5-base\", args=model_args)\n",
    "\n",
    "model_args = T5Args()\n",
    "model_args.overwrite_output_dir = True  # Allow overwriting the output directory\n",
    "model_args.num_train_epochs = 200\n",
    "model_args.no_save = True\n",
    "model_args.use_cuda = False  # Set before initializing the model\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "# Explicitly pass use_cuda=False in the constructor\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "def count_matches(labels, preds):\n",
    "    print(labels)\n",
    "    print(preds)\n",
    "    return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n",
    "\n",
    "\n",
    "model.train_model(train_df, eval_data=eval_df, matches=count_matches)\n",
    "\n",
    "print(model.eval_model(eval_df, matches=count_matches))\n",
    "\n",
    "##Try pip install --upgrade transformers simpletransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ebd26f-20cd-46a3-aeb1-444fd087b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6a2606b69a40ceb1df40e21d19afcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1282\n",
      "INFO:simpletransformers.t5.t5_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41866f9293d4061a731427b92bbd5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8804946eda4c9bbf1c5799afd49a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 200:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24e0c0202de4f5caff6fe56506df158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1282\n",
      "INFO:simpletransformers.t5.t5_model:{'eval_loss': 6.388903617858887}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m pred \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels, preds)])\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount_matches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     54\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval_model(eval_df, matches\u001b[38;5;241m=\u001b[39mcount_matches)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:242\u001b[0m, in \u001b[0;36mT5Model.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, qrels, run_dict, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(train_data, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    240\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:803\u001b[0m, in \u001b[0;36mT5Model.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_data, qrels, run_dict, eval_name, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    802\u001b[0m     training_progress_scores[key]\u001b[38;5;241m.\u001b[39mappend(results[key])\n\u001b[1;32m--> 803\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_progress_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m report\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m    805\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_progress_scores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    806\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    807\u001b[0m )\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mwandb_project \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sweeping:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# pip install simpletransformers\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize logging and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", legacy=False)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Example training data\n",
    "train_data = [\n",
    "    [\"binary classification\", \"Anakin was Luke's father\", \"1\"],\n",
    "    [\"binary classification\", \"Luke was a Sith Lord\", \"0\"]]\n",
    "   # [\"generate question\", \"Star Wars is an American epic space-opera media franchise created by George Lucas, which began with the eponymous 1977 film and quickly became a worldwide pop-culture phenomenon\", \"Who created the Star Wars franchise?\"],\n",
    "   # [\"generate question\", \"Anakin was Luke's father\", \"Who was Luke's father?\"],\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Example evaluation data\n",
    "eval_data = [\n",
    "    [\"binary classification\", \"Leia was Luke's sister\", \"1\"],\n",
    "    [\"binary classification\", \"Han was a Sith Lord\", \"0\"]]\n",
    "   # [\"generate question\", \"In 2020, the Star Wars franchise's total value was estimated at US$70 billion, and it is currently the fifth-highest-grossing media franchise of all time.\", \"What is the total value of the Star Wars franchise?\"],\n",
    "   # [\"generate question\", \"Leia was Luke's sister\", \"Who was Luke's sister?\"],\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Model configuration\n",
    "model_args = T5Args()\n",
    "model_args.overwrite_output_dir = True  # Allow overwriting the output directory\n",
    "model_args.num_train_epochs = 200\n",
    "model_args.no_save = True\n",
    "model_args.use_cuda = False  # Set before initializing the model\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "# Initialize the T5 model\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "# Define custom evaluation function\n",
    "def count_matches(labels, preds):\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Predictions:\", preds)\n",
    "    return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, eval_data=eval_df, matches=count_matches)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = model.eval_model(eval_df, matches=count_matches)\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34849c-0b39-41c1-b015-211158fb3575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320b1b8-084c-49e9-93d5-af7d0861181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# pip install simpletransformers\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize logging and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", legacy=False)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Example training data (binary classification task)\n",
    "train_data = [\n",
    "    [\"binary classification\", \"Anakin was Luke's father\", \"1\"],\n",
    "    [\"binary classification\", \"Luke was a Sith Lord\", \"0\"]\n",
    "]\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Example evaluation data (binary classification task)\n",
    "eval_data = [\n",
    "    [\"binary classification\", \"Leia was Luke's sister\", \"1\"],\n",
    "    [\"binary classification\", \"Han was a Sith Lord\", \"0\"]\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Model configuration\n",
    "model_args = T5Args()\n",
    "model_args.overwrite_output_dir = True  # Allow overwriting the output directory\n",
    "model_args.num_train_epochs = 200\n",
    "model_args.no_save = True\n",
    "model_args.use_cuda = False  # Set before initializing the model\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "# Initialize the T5 model\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "# Define custom evaluation function for binary classification\n",
    "def count_matches(labels, preds):\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Predictions:\", preds)\n",
    "    return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model.train_model(train_df, eval_data=eval_df, matches=count_matches)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = model.eval_model(eval_df, matches=count_matches)\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0701b477-9ff9-4e90-86db-e05da19cc122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame shape: (2, 3)\n",
      "Evaluation DataFrame shape: (2, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280e531d76894de995f8b12bbeda0bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1282\n",
      "INFO:simpletransformers.t5.t5_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4a0c047c274974941d40c0bd9c632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eb8f6c23e7419980ade539f4e38524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 200:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fa3755c727434281a337f40052cabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1282\n",
      "INFO:simpletransformers.t5.t5_model:{'eval_loss': 6.388903617858887}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount_matches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:242\u001b[0m, in \u001b[0;36mT5Model.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, qrels, run_dict, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(train_data, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    240\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:803\u001b[0m, in \u001b[0;36mT5Model.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_data, qrels, run_dict, eval_name, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    802\u001b[0m     training_progress_scores[key]\u001b[38;5;241m.\u001b[39mappend(results[key])\n\u001b[1;32m--> 803\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_progress_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m report\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m    805\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_progress_scores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    806\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    807\u001b[0m )\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mwandb_project \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sweeping:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize logging and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", legacy=False)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Example training data (binary classification task)\n",
    "train_data = [\n",
    "    [\"binary classification\", \"Anakin was Luke's father\", \"1\"],\n",
    "    [\"binary classification\", \"Luke was a Sith Lord\", \"0\"]\n",
    "]\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Example evaluation data (binary classification task)\n",
    "eval_data = [\n",
    "    [\"binary classification\", \"Leia was Luke's sister\", \"1\"],\n",
    "    [\"binary classification\", \"Han was a Sith Lord\", \"0\"]\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Ensure the target_text column is of the correct data type (str)\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].astype(str)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].astype(str)\n",
    "\n",
    "# Check that both DataFrames have the same number of rows and columns\n",
    "print(\"Training DataFrame shape:\", train_df.shape)\n",
    "print(\"Evaluation DataFrame shape:\", eval_df.shape)\n",
    "\n",
    "# Model configuration\n",
    "model_args = T5Args()\n",
    "model_args.overwrite_output_dir = True  # Allow overwriting the output directory\n",
    "model_args.num_train_epochs = 200\n",
    "model_args.no_save = True\n",
    "model_args.use_cuda = False  # Set before initializing the model\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "# Initialize the T5 model\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "# Define custom evaluation function for binary classification\n",
    "def count_matches(labels, preds):\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Predictions:\", preds)\n",
    "    return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model.train_model(train_df, eval_data=eval_df, matches=count_matches)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = model.eval_model(eval_df, matches=count_matches)\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd5814a-3ece-47ba-9c99-017efaff04e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame shape: (2, 3)\n",
      "Evaluation DataFrame shape: (2, 3)\n",
      "Missing values in train_df:\n",
      " prefix         0\n",
      "input_text     0\n",
      "target_text    0\n",
      "dtype: int64\n",
      "Missing values in eval_df:\n",
      " prefix         0\n",
      "input_text     0\n",
      "target_text    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97166be206d1488186fc33a8671111bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1282\n",
      "INFO:simpletransformers.t5.t5_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9359c4ab26049afae6d4b17b41a7efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ecae38a79b470d816d2c005dcace9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 200:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a62a6d40a74e1e92c72010e24e15f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1282\n",
      "INFO:simpletransformers.t5.t5_model:{'eval_loss': 6.388903617858887}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount_matches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:242\u001b[0m, in \u001b[0;36mT5Model.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, qrels, run_dict, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(train_data, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    240\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:803\u001b[0m, in \u001b[0;36mT5Model.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_data, qrels, run_dict, eval_name, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    802\u001b[0m     training_progress_scores[key]\u001b[38;5;241m.\u001b[39mappend(results[key])\n\u001b[1;32m--> 803\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_progress_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m report\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m    805\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_progress_scores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    806\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    807\u001b[0m )\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mwandb_project \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sweeping:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize logging and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", legacy=False)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Example training data (binary classification task)\n",
    "train_data = [\n",
    "    [\"binary classification\", \"Anakin was Luke's father\", \"1\"],\n",
    "    [\"binary classification\", \"Luke was a Sith Lord\", \"0\"]\n",
    "]\n",
    "train_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Example evaluation data (binary classification task)\n",
    "eval_data = [\n",
    "    [\"binary classification\", \"Leia was Luke's sister\", \"1\"],\n",
    "    [\"binary classification\", \"Han was a Sith Lord\", \"0\"]\n",
    "]\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Ensure the target_text column is of the correct data type (str)\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].astype(str)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].astype(str)\n",
    "\n",
    "# Check the shape of both train_df and eval_df\n",
    "print(\"Training DataFrame shape:\", train_df.shape)\n",
    "print(\"Evaluation DataFrame shape:\", eval_df.shape)\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(\"Missing values in train_df:\\n\", train_df.isnull().sum())\n",
    "print(\"Missing values in eval_df:\\n\", eval_df.isnull().sum())\n",
    "\n",
    "# Check that both DataFrames have the same number of rows and columns\n",
    "assert train_df.shape[0] == eval_df.shape[0], \"Training and evaluation datasets must have the same number of rows.\"\n",
    "\n",
    "# Model configuration\n",
    "model_args = T5Args()\n",
    "model_args.overwrite_output_dir = True  # Allow overwriting the output directory\n",
    "model_args.num_train_epochs = 200\n",
    "model_args.no_save = True\n",
    "model_args.use_cuda = False  # Set before initializing the model\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "# Initialize the T5 model\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "# Define custom evaluation function for binary classification\n",
    "def count_matches(labels, preds):\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Predictions:\", preds)\n",
    "    return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model.train_model(train_df, eval_data=eval_df, matches=count_matches)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = model.eval_model(eval_df, matches=count_matches)\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e761e0d-2a29-4194-bbbe-3bc3db106aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23781ff5f2e483b81b2c9126521998d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1286\n",
      "INFO:simpletransformers.t5.t5_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5c7ce9f4124d82804c8045300d9c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99936dda0ddf4042a4b78db87389884a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32504d7220f543f8aeb94aaddf35d9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-base_cached_1284\n",
      "INFO:simpletransformers.t5.t5_model:{'eval_loss': 5.338079452514648}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount_matches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:242\u001b[0m, in \u001b[0;36mT5Model.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, qrels, run_dict, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(train_data, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    240\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\simpletransformers\\t5\\t5_model.py:803\u001b[0m, in \u001b[0;36mT5Model.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_data, qrels, run_dict, eval_name, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    802\u001b[0m     training_progress_scores[key]\u001b[38;5;241m.\u001b[39mappend(results[key])\n\u001b[1;32m--> 803\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_progress_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m report\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m    805\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_progress_scores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    806\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    807\u001b[0m )\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mwandb_project \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sweeping:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize logging and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", legacy=False)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Example training data for binary classification\n",
    "train_data = [\n",
    "    [\"binary classification\", \"I love this movie\", \"1\"],   # Positive sentiment\n",
    "    [\"binary classification\", \"This movie is terrible\", \"0\"],  # Negative sentiment\n",
    "    [\"binary classification\", \"The food was great\", \"1\"],    # Positive sentiment\n",
    "    [\"binary classification\", \"I hate waiting\", \"0\"],       # Negative sentiment\n",
    "    [\"binary classification\", \"This is a great product\", \"1\"],  # Positive sentiment\n",
    "    [\"binary classification\", \"I am not happy with this\", \"0\"],  # Negative sentiment\n",
    "]\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Example evaluation data for binary classification\n",
    "eval_data = [\n",
    "    [\"binary classification\", \"I had a wonderful day\", \"1\"],  # Positive sentiment\n",
    "    [\"binary classification\", \"I do not like this movie\", \"0\"],  # Negative sentiment\n",
    "    [\"binary classification\", \"Fantastic service\", \"1\"],   # Positive sentiment\n",
    "    [\"binary classification\", \"The worst experience\", \"0\"],  # Negative sentiment\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Model configuration\n",
    "model_args = T5Args()\n",
    "model_args.overwrite_output_dir = True  # Allow overwriting the output directory\n",
    "model_args.num_train_epochs = 3  # Set a smaller number of epochs for faster training\n",
    "model_args.no_save = True\n",
    "model_args.use_cuda = False  # Set to True if using GPU\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "# Initialize the T5 model\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "# Define custom evaluation function for binary classification\n",
    "def count_matches(labels, preds):\n",
    "    return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model.train_model(train_df, eval_data=eval_df, matches=count_matches)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = model.eval_model(eval_df, matches=count_matches)\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b13889-4114-4f7b-b8a1-263b435755f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 1/1 [00:20<00:00, 20.38s/it, loss=11]\n",
      "Epoch 2: 100%|| 1/1 [00:04<00:00,  4.79s/it, loss=8.67]\n",
      "Epoch 3: 100%|| 1/1 [00:03<00:00,  3.46s/it, loss=9.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Example binary classification dataset (positive or negative sentiment)\n",
    "train_data = [\n",
    "    [\"I love this movie\", 1],\n",
    "    [\"This movie is terrible\", 0],\n",
    "    [\"The food was great\", 1],\n",
    "    [\"I hate waiting\", 0],\n",
    "    [\"This is a great product\", 1],\n",
    "    [\"I am not happy with this\", 0]\n",
    "]\n",
    "\n",
    "# Convert the data into a pandas DataFrame\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"text\"].tolist(), train_df[\"label\"].tolist(), test_size=0.2\n",
    ")\n",
    "\n",
    "# Tokenize the inputs and labels\n",
    "def tokenize_function(texts, labels):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=64)\n",
    "    labels_encodings = tokenizer([f\"{label}\" for label in labels], padding=True, truncation=True, return_tensors=\"pt\", max_length=2)\n",
    "    return encodings, labels_encodings.input_ids\n",
    "\n",
    "train_encodings, train_labels_encodings = tokenize_function(train_texts, train_labels)\n",
    "val_encodings, val_labels_encodings = tokenize_function(val_texts, val_labels)\n",
    "\n",
    "# Convert the encodings into a DataLoader\n",
    "class BinaryClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = BinaryClassificationDataset(train_encodings, train_labels_encodings)\n",
    "val_dataset = BinaryClassificationDataset(val_encodings, val_labels_encodings)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Set up optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Set the number of epochs\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == batch['labels']).sum().item()\n",
    "        total += batch['labels'].size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b0cb6fa-4625-4b1c-9c8e-c8d9d751d7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "\n",
      "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\n",
      "Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
      "Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\n",
      "As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.\n",
      "\n",
      "\n",
      "Generated Summary: \n",
      "leading AI textbooks define the field as the study of \"intelligent agents\" the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions . as machines become increasingly capable, tasks considered to require \"intelligence\" are often removed .\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Example text for summarization\n",
    "input_text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\n",
    "Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
    "Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\n",
    "As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the input with the summarization prefix\n",
    "input_text_with_prefix = \"summarize: \" + input_text\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text_with_prefix, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate summary with the model\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the summary\n",
    "print(\"Original Text: \")\n",
    "print(input_text)\n",
    "print(\"\\nGenerated Summary: \")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fe017-038c-4ded-bff5-726b8de81fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
