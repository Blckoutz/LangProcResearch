{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fe461-1ca2-4449-808b-08d072e679cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an abstract\n",
    "import requests\n",
    "\n",
    "#capitalization matters\n",
    "#can enter any wikipedia title here\n",
    "#subject = 'Conway Twitty'\n",
    "#emperor penguin\n",
    "#java\n",
    "#java (programming language)\n",
    "#python (programming language)\n",
    "#python\n",
    "#Pythonidae\n",
    "#History of Python\n",
    "subject = input(\"What is the subject?\")\n",
    "\n",
    "\n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': subject,\n",
    "        'prop': 'extracts',\n",
    "        'exintro': True,\n",
    "        'explaintext': True,\n",
    "    }\n",
    " \n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    " \n",
    "page = next(iter(data['query']['pages'].values()))\n",
    "#print(page)\n",
    "#Show only the abstract\n",
    "print(page['extract'][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b942e93-9c19-46e9-adc3-ee1652e446ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get whole text \n",
    "import requests\n",
    "from lxml import html\n",
    " \n",
    "#subject = 'Python (programming language)'\n",
    "subject = input(\"What is the subject?\")\n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "params = {\n",
    "        'action': 'parse',\n",
    "        'format': 'json',\n",
    "        'page': subject,\n",
    "        'prop': 'text',\n",
    "        'redirects':''\n",
    "    }\n",
    "  \n",
    "response = requests.get(url, params=params).json()\n",
    "raw_html = response['parse']['text']['*']\n",
    "#print(raw_html)\n",
    "document = html.document_fromstring(raw_html)\n",
    "#print(document) \n",
    "text = ''\n",
    "for p in document.xpath('//p'):\n",
    "   text += p.text_content() + '\\n'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18304208-aea5-4229-967e-4cfa88cff30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a search\n",
    "#find occurrences and check if an entity exists and what is the subject name\n",
    "import requests\n",
    " \n",
    "#query = 'python'\n",
    "#not case sensitive\n",
    "query = input(\"What is the query?\") \n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "params = {\n",
    "            'action':'query',\n",
    "            'format':'json',\n",
    "            'list':'search',\n",
    "            'utf8':1,\n",
    "            'srsearch':query\n",
    "        }\n",
    " \n",
    "data = requests.get(url, params=params).json()\n",
    "#print(data) \n",
    "for i in data['query']['search']:\n",
    "    print(i['title'], ' - Word count: ', i['wordcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021805a-794d-42e6-92db-231381677ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "subject = input(\"What is the subject?\") \n",
    " \n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "params = {\n",
    "            'action': 'parse',\n",
    "            'page': subject,\n",
    "            'format': 'json',\n",
    "            'prop':'text',\n",
    "            'redirects':''\n",
    "        }\n",
    " \n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    " \n",
    "raw_html = data['parse']['text']['*']\n",
    "soup = BeautifulSoup(raw_html,'html.parser')\n",
    "soup.find_all('p') #find all paragraphs\n",
    "text = ''\n",
    " \n",
    "for p in soup.find_all('p'):\n",
    "    text += p.text\n",
    " \n",
    "print(text[:60])\n",
    "print('Text length: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597a650-948a-494d-ab4f-bdfcd38fb1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all links on the page\n",
    "#get titles and count\n",
    "import requests\n",
    " \n",
    "subject = input('What is the subject?')\n",
    " \n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    " \n",
    "params = {\n",
    "    'action': 'query',\n",
    "    'format': 'json',\n",
    "    'titles': subject,\n",
    "    'prop': 'links',\n",
    "    'pllimit': 'max',\n",
    "    'redirects':''\n",
    "}\n",
    " \n",
    "response = requests.get(url=url, params=params)\n",
    "data = response.json()\n",
    " \n",
    "pages = data['query']['pages']\n",
    "page = 1\n",
    "page_titles = []\n",
    " \n",
    "for key, val in pages.items():\n",
    "    for link in val['links']:\n",
    "        page_titles.append(link['title'])\n",
    " \n",
    "while 'continue' in data:\n",
    "    plcontinue = data['continue']['plcontinue']\n",
    "    params['plcontinue'] = plcontinue\n",
    " \n",
    "    response = requests.get(url=url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    " \n",
    "    page += 1\n",
    " \n",
    "    for key, val in pages.items():\n",
    "        for link in val['links']:\n",
    "            page_titles.append(link['title'])\n",
    " \n",
    "print(page_titles[:3])\n",
    "print('number of links: ',len(page_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda900a2-fbe2-4d83-8040-b25c4e7bf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix with pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    " \n",
    "subject = input('What subject?')#'Machine Learning'\n",
    " \n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "params = {\n",
    "            'action':'parse',\n",
    "            'prop':'text',\n",
    "            'format':'json',\n",
    "            'page':subject,\n",
    "            'section':0,\n",
    "            'redirects':''\n",
    "        }\n",
    "data = requests.get(url, params=params).json()\n",
    " \n",
    "soup = BeautifulSoup(data['parse']['text']['*'],'html.parser')\n",
    " \n",
    "infobox = soup.find('table',{'class':'infobox','class':'sidebar'})\n",
    " \n",
    "a_tags = infobox.find_all('a', href=True)\n",
    " \n",
    "links = []\n",
    "for tag in a_tags:\n",
    "    if not tag.text == '' and 'wiki' in tag['href']:\n",
    "        links.append({'anchor':tag.text, 'href':tag['href']})\n",
    " \n",
    "pd.DataFrame(links).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3642f35-3196-4dcc-83c5-a019f63eb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "\n",
    "##Clarify the one we need\n",
    "subject=input('What subject do you want information on?')\n",
    "#1. check the wiki name\n",
    "#make a search\n",
    "#find occurrences and check if an entity exists and what is the subject name\n",
    "params = {\n",
    "            'action':'query',\n",
    "            'format':'json',\n",
    "            'list':'search',\n",
    "            'utf8':1,\n",
    "            'srsearch':subject\n",
    "        }\n",
    " \n",
    "data = requests.get(url, params=params).json()\n",
    "#print(len(data['query']['search']))\n",
    "subjectConfirmed=0\n",
    "count=0\n",
    "if len(data['query']['search'])> 1:\n",
    "    for i in data['query']['search']:\n",
    "        print('CHOICE',count,i['title'], ' - Word count: ', i['wordcount'])\n",
    "        count=count+1\n",
    "    subjectConfirmed=input('Please enter the number of the topic you were requesting')\n",
    "    subject=data['query']['search'][int(subjectConfirmed)]['title']\n",
    "#print(subjectConfirmed)    \n",
    "#print(data['query']['search'][int(subjectConfirmed)]['title'])\n",
    "\n",
    "#2. grab the document\n",
    "params = {\n",
    "        'action': 'parse',\n",
    "        'format': 'json',\n",
    "        'page': subject,\n",
    "        'prop': 'text',\n",
    "        'redirects':''\n",
    "    }\n",
    "  \n",
    "response = requests.get(url, params=params).json()\n",
    "raw_html = response['parse']['text']['*']\n",
    "#print(raw_html)\n",
    "document = html.document_fromstring(raw_html)\n",
    "#print(document) \n",
    "text = ''\n",
    "for p in document.xpath('//p'):\n",
    "   text += p.text_content() + '\\n'\n",
    "#print(len(text))\n",
    "#print(text[:50])\n",
    "\n",
    "#3. now we can store this data, process it, etc.\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords #stopword handling\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "words=word_tokenize(text) \n",
    "sentences=sent_tokenize(text)\n",
    "#TODO: Decide on a tokenizer\n",
    "print('# words',len(words))#has commas, ( etc.\n",
    "print('# sentences',len(sentences))\n",
    "withoutStops=[word for word in words if word not in english_stops]\n",
    "print('# words without stop words',len(withoutStops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43315c84-3bb8-4258-9a7b-40eacac68eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
